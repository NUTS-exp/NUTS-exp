{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from date_selection import get_date_graph, pagerank, get_dates_perso\n",
    "from textrank import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "import _pickle as cPickle\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import math\n",
    "import numpy\n",
    "import timeit\n",
    "\n",
    "from tilse.data import timelines\n",
    "from tilse.evaluation import rouge\n",
    "from joblib import Parallel, delayed\n",
    "from gensim.summarization.textcleaner import clean_text_by_word as _clean_text_by_word\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crisis dataset from http://l3s.de/~gtran/timeline/\n",
    "DATA_PATH = None\n",
    "\n",
    "# temporally tagged sentences for date selection by HeidelTime\n",
    "# cPickle files: e.g. egypt.dated_sents\n",
    "#                     containing   [[(pub_date_sent1, sent1), (ref_date1_sent1, sent1), (ref_date2_sent1, sent1)], [(pub_date_sent2, sent2)], ...]\n",
    "SENT_PATH = None\n",
    "\n",
    "# use exactly the same sentence corpus per timeline with TILSE for extractive summarization\n",
    "# from `sentence_preprocessing.ipynb`\n",
    "# cPickle file: tssf_crisis.filtered_sents\n",
    "#               containing {'egypt': {'date1': [sents], {'date2': [sents]}, ...},\n",
    "#                           'syria': {'date1': [sents], {'date2': [sents]}, ...},\n",
    "#                            ... }\n",
    "filtered_dated_sents = None\n",
    "\n",
    "def get_groundtruth(tl):\n",
    "    gt = []\n",
    "    p = f'{DATA_PATH}/{tl}/public/timelines'\n",
    "    for pp in os.listdir(p):\n",
    "        with codecs.open(f'{p}/{pp}', 'r', \"utf-8\", \"ignore\") as f:\n",
    "            gt.append(timelines.Timeline.from_file(f))\n",
    "    groundtruth = timelines.GroundTruth(gt)\n",
    "    return groundtruth\n",
    "\n",
    "def get_dt_sents(tl):\n",
    "    return filtered_dated_sents[tl]\n",
    "\n",
    "def get_daily_summarization(dat, sents, SENT_NUM):\n",
    "    sents = summarize('\\n\\n'.join(sents), num=SENT_NUM, split=True, rerank=False)\n",
    "    return (dat, sents[:SENT_NUM])\n",
    "\n",
    "def tokenize_sents(dated_sent):\n",
    "    dt, sent = dated_sent[0]\n",
    "    sent = _clean_text_by_word(sent)\n",
    "    return [sent[word].token for word in sent]\n",
    "\n",
    "def _cos(v1, v2):\n",
    "    v1 = dict(v1)\n",
    "    v2 = dict(v2)\n",
    "    norm1 = numpy.sqrt(sum([v1[i] ** 2 for i in v1]))\n",
    "    norm2 = numpy.sqrt(sum([v2[i] ** 2 for i in v2]))\n",
    "    return sum([v1[i] * v2[i] for i in v1 if i in v2]) / norm1 / norm2\n",
    "\n",
    "def get_timeline(tl, timeline, perso=False, postprocess=False):\n",
    "    \n",
    "    dated_sents = cPickle.load(open(f'{SENT_PATH}/{tl}.dated_sents', 'rb'))\n",
    "\n",
    "    dts = timeline.get_dates()\n",
    "    date_range = (min(dts), max(dts))\n",
    "    pred_date_range = list(date_range)\n",
    "    for i in dated_sents:\n",
    "        if len(i[0][0]) == 10:\n",
    "            pred_date_range[0] = min(pred_date_range[0], datetime.strptime(i[0][0], '%Y-%m-%d').date())\n",
    "            pred_date_range[1] = max(pred_date_range[1], datetime.strptime(i[0][0], '%Y-%m-%d').date())\n",
    "    potential_dates = set()\n",
    "    st = pred_date_range[0]\n",
    "    while st <= pred_date_range[1]:\n",
    "        potential_dates.add(st.strftime('%Y-%m-%d'))\n",
    "        st += timedelta(days=1)\n",
    "\n",
    "    G = get_date_graph(dated_sents, potential_dates)\n",
    "    dt_sents = get_dt_sents(tl)\n",
    "    DATE_NUM = len(timeline)\n",
    "    SENT_NUM = math.floor(numpy.mean([len(timeline[i]) for i in timeline]))\n",
    "    \n",
    "    if perso:\n",
    "        betas = []\n",
    "        for _beta in range(1000):\n",
    "            betas.append(0.00001 * _beta)\n",
    "        \n",
    "        res = Parallel(n_jobs=23, backend='multiprocessing')(delayed(get_dates_perso)(G, beta, pred_date_range, date_range, dt_sents, DATE_NUM) for beta in betas)\n",
    "        dts = min(res, key=lambda x: x[0])[1]\n",
    "    else:\n",
    "        res = pagerank(G)\n",
    "        dts = get_dates(res, date_range, dt_sents, DATE_NUM)\n",
    "\n",
    "    tmp = {}\n",
    "    for dt in dts:\n",
    "        t = dt.strftime('%Y-%m-%d')\n",
    "        if date_range[0] <= dt and dt <= date_range[1] and t in dt_sents:\n",
    "            sents = list(set(dt_sents[t]))\n",
    "            tmp.setdefault(dt, sents)\n",
    "    \n",
    "    if postprocess:\n",
    "        ttmp = Parallel(n_jobs=23, backend='multiprocessing')(delayed(get_daily_summarization)(dat, sents, SENT_NUM * 10) for dat, sents in tmp.items())\n",
    "        \n",
    "        sents = Parallel(n_jobs=23, backend='multiprocessing')(delayed(tokenize_sents)(dated_sent) for dated_sent in dated_sents)\n",
    "        dictionary = Dictionary(sents)\n",
    "        bow = [dictionary.doc2bow(sent) for sent in sents]\n",
    "        model = TfidfModel(bow)\n",
    "        \n",
    "        vecs = []\n",
    "        tmp = {}\n",
    "        for dt, sents in ttmp:\n",
    "            tmp.setdefault(dt, [])\n",
    "            sent = sents[0]\n",
    "            vec = _clean_text_by_word(sent)\n",
    "            vec = dictionary.doc2bow(vec)\n",
    "            vec = model[vec]\n",
    "            vecs.append(vec)\n",
    "            tmp[dt].append(sent)\n",
    "\n",
    "        for dt, sents in ttmp:\n",
    "            tmp.setdefault(dt, [])\n",
    "            cnt = 1\n",
    "            for sent in sents[1:]:\n",
    "                if cnt >= SENT_NUM:\n",
    "                    break\n",
    "                vec = _clean_text_by_word(sent)\n",
    "                vec = dictionary.doc2bow(vec)\n",
    "                vec = model[vec]\n",
    "                sim = max(_cos(vec, i) for i in vecs)\n",
    "                if sim < 0.5:\n",
    "                    tmp[dt].append(sent)\n",
    "                    cnt += 1\n",
    "                    vecs.append(vec)\n",
    "\n",
    "    else:\n",
    "        tmp = Parallel(n_jobs=23, backend='multiprocessing')(delayed(get_daily_summarization)(dat, sents, SENT_NUM) for dat, sents in tmp.items())\n",
    "        tmp = dict(tmp)\n",
    "    \n",
    "    predicted_timeline = timelines.Timeline(tmp)\n",
    "    return (predicted_timeline, timelines.GroundTruth([timeline]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_timelines = []\n",
    "for tl in os.listdir(DATA_PATH):\n",
    "    gt = get_groundtruth(tl)\n",
    "    for timeline in gt.timelines:\n",
    "        tl_timelines.append((tl, timeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sec:  632.6240723449737\n",
      "avg sec. per timline:  28.75563965204426\n"
     ]
    }
   ],
   "source": [
    "eval_pairs = []\n",
    "sentnum_time = []\n",
    "\n",
    "start_time1 = timeit.default_timer()\n",
    "for tl, timeline in tl_timelines:\n",
    "    start_time = timeit.default_timer()\n",
    "    eval_pairs.append(get_timeline(tl, timeline, perso=True, postprocess=True))\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    dt_sents = get_dt_sents(tl)\n",
    "    sentnum_time.append(\n",
    "        (sum([len(dt_sents[dt]) for dt in dt_sents]), elapsed)\n",
    "    )\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time1\n",
    "print('total sec: ', elapsed)\n",
    "print('avg sec. per timline: ', elapsed / len(eval_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge 1 f1': 0.36051176245962313, 'rouge 2 f1': 0.0759064124038041}\n",
      "{'rouge 1 f1': 0.06787000464523299, 'rouge 2 f1': 0.02025320688329175}\n",
      "{'rouge 1 f1': 0.08460087822235977, 'rouge 2 f1': 0.023219421029152542}\n"
     ]
    }
   ],
   "source": [
    "def get_rouge_score(eval_pairs, c=0):\n",
    "    evaluator = rouge.TimelineRougeEvaluator(measures=[\"rouge_1\", \"rouge_2\"])\n",
    "    res = []\n",
    "    if c == 0:\n",
    "        res = Parallel(n_jobs=23)(delayed(evaluator.evaluate_concat)(t1, t2) for t1, t2 in eval_pairs)\n",
    "    elif c == 1:\n",
    "        res = Parallel(n_jobs=23)(delayed(evaluator.evaluate_agreement)(t1, t2) for t1, t2 in eval_pairs)\n",
    "    elif c == 2:\n",
    "        res = Parallel(n_jobs=23)(delayed(evaluator.evaluate_align_date_content_costs_many_to_one)(t1, t2) for t1, t2 in eval_pairs)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    pre = numpy.mean([i['rouge_1']['precision'] for i in res])\n",
    "    rec = numpy.mean([i['rouge_1']['recall'] for i in res])\n",
    "    rouge_1_f = 2 * pre * rec / (pre + rec)\n",
    "    \n",
    "    pre = numpy.mean([i['rouge_2']['precision'] for i in res])\n",
    "    rec = numpy.mean([i['rouge_2']['recall'] for i in res])\n",
    "    rouge_2_f = 2 * pre * rec / (pre + rec)\n",
    "    return {'rouge 1 f1': rouge_1_f,\n",
    "            'rouge 2 f1': rouge_2_f}\n",
    "\n",
    "print(get_rouge_score(eval_pairs[:], c=0))\n",
    "print(get_rouge_score(eval_pairs[:], c=1))\n",
    "print(get_rouge_score(eval_pairs[:], c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.27562968799077764, 0.27562968799077764)\n"
     ]
    }
   ],
   "source": [
    "def get_date_f1(eval_pairs):\n",
    "    pre = []\n",
    "    rec = []\n",
    "    f = []\n",
    "    for t1, t2 in eval_pairs:\n",
    "        d1 = set([i for i in t1])\n",
    "        d2 = set([i for i in t2.timelines[0]])\n",
    "        pre.append(len(d1 & d2) / len(d1))\n",
    "        rec.append(len(d1 & d2) / len(d2))\n",
    "        if pre[-1] + rec[-1] != 0:\n",
    "            f.append(2 * pre[-1] * rec[-1] / (pre[-1] + rec[-1]))\n",
    "        else:\n",
    "            f.append(0)\n",
    "    pre = numpy.mean(pre)\n",
    "    rec = numpy.mean(rec)\n",
    "    f = numpy.mean(f)\n",
    "    f1 = 2 * pre * rec / (pre + rec)\n",
    "    return f1, f\n",
    "\n",
    "print(get_date_f1(eval_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sanity check for statistics of sentence number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643\n",
      "1.181959564541213\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "sentence_cnt = []\n",
    "for t1, t2 in eval_pairs:\n",
    "    for dt in t1:\n",
    "        sentence_cnt.append(len(t1[dt]))\n",
    "print(len(sentence_cnt))\n",
    "print(numpy.mean(sentence_cnt))\n",
    "print(numpy.median(sentence_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "1.1818181818181819 0.385694607919935\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "sentence_cnt = []\n",
    "for t1, t2 in eval_pairs:\n",
    "    s = 0\n",
    "    for dt in t1:\n",
    "        s = max(s, len(t1[dt]))\n",
    "    sentence_cnt.append(s)\n",
    "print(len(sentence_cnt))\n",
    "print(numpy.mean(sentence_cnt), numpy.std(sentence_cnt))\n",
    "print(numpy.median(sentence_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
